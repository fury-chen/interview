# 万兴科技面经

- 自我介绍

- 介绍一下这几部分工作

- 平时用什么语言比较多？

  - 平时使用C++比较多

  - 对于虚函数了解吗？

  - 如果子类指向两个父类都有虚函数时，虚函数是怎么实现的？

    ```C++
    class A {  
    public:  
        virtual void func() {}  
    };  
    
    class B {  
    public:  
        virtual void func() {}  
    };  
    
    class C : public A, public B {  
    public:  
        void func() override {} // 这个 func 可以重写 A 或 B 的 func  
    };
    //这种情况下，类C的虚函数可能包含指向A和B的func函数地址，在C实例中会有两个指针，分别指向A的虚函数表和B的虚函数表
    //如果重写了父类的虚函数，C的虚函数表就会更新，包含一个指向重写func函数的地址，可能会有两个指针分别指向AB的虚函数表
    ```

    

  - 是否了解vec？如果我想释放一个vec对象的内存而不delete这个对象应该怎么做？
    - GPT：可以使用clear()方法移除容器当中的所有元素
  - 比如一个vec[10000]我想释放这段内存而不析构这个vec对象应该怎么做？

- 你实习经历当中的GEMM优化是怎么做的？

  - 介绍Naive
  - 除了Naive之外还怎么优化了？

- LLM相关

  - LLM使用的时候了解SA吗？
  - SA当中的几个参数清楚吗？d_k是做什么用的？
  - LLM当中token映射是怎么做的？了解过吗？Word2Vec的相关内容
  - 

- 我觉得你应该还要再去了解一下GEMM

- 总结：基本都是围绕简历的相关内容，但是总体来说答的不是特别好，准备错方向了，并且自己基础不够硬，非常规问题答不上来，常规问题答的不够好

  

- 补充扩展：

  - 虚函数的实现：
    - 定义：在基类当中声明为virtual并且在一个或多个派生类中被重新定义的成员函数
    - 作用：主要是实现了多态机制（用父类别的指针指向
    - 实现方法：虚函数表+虚表指针
      - 编译器实现：为每个类对象添加一个隐藏成员，隐藏成员保存了一个指向函数的数组指针，成为虚函数指针；这种数组称为虚函数表，每个类使用一个虚函数表，每个类对象用一个虚函数
      - 即：基类对象包含一个虚表指针，指向基类当中所有虚函数的地址表，派生类对象也包含一个虚表指针，指向派生类的虚函数表；如果派生类重写了基类的虚方法，派生类虚函数表将保存重写的虚函数地址，而不是基类的虚函数地址

  - tokenize详解：[深度学习文本预处理利器：Tokenizer详解-CSDN博客](https://blog.csdn.net/lsb2002/article/details/133095184)
    - 英文文本向量化：默认情况下删除所有标点符号，将文本转换为空格分隔的单词序列，这些序列被分割成标记列表，然后它们被索引或者向量化
    - tokenizer库其实就是收集原始数据当中的语料，按照一定规则分开，为后续的Embedding做准备（[Huggingface详细教程之Tokenizer库 - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/591335566)）
      - 先处理文本将其变为列表（Norm+Pre-tokenization=文本清洗删除多余换行、空格+将句子且分为词元）
      - 通过模型处理subword分词+各种标签（Model+PostProcesser=基于不同分词模型的切分+添加特殊分词逻辑）![img](https://pica.zhimg.com/80/v2-92b4e84a39d6d032c6da190898398950_720w.webp)

  - HuggingFace中的transform库包含三个核心类：configuration、models、tokenizer

  - 分词器的基本函数：init train encode decode（[编码实践 | 一文读懂Tokenizer | 以Word-based 和 Byte Pair Encoding为例 | YuyaoGe's Website (geyuyao.com)](https://geyuyao.com/post/tokenizer/)）

  - [Tokenizer的系统梳理，并手推每个方法的具体实现-腾讯云开发者社区-腾讯云 (tencent.com)](https://cloud.tencent.com/developer/article/2327739)

- SA基础知识补充：[This post is all you need（上卷）——层层剥开Transformer - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/420820453?s_r=0)

  - 计算公式：![图片](https://i-blog.csdnimg.cn/blog_migrate/1af6d01544dba21619bebd47373e4b5a.png)
  - 什么是SA？——注意力机制可以描述为将query和一系列key-value对映射到某个输出的过程，这个输出的向量就是根据query和key计算得到的权重作用于value权重和
    - 根据公式可以看出注意力的核心过程就是通过QK计算得到注意力权重；然后再作用于V得到整个权重和输出（面试回答Attn计算公式后，补充softmax部分是权重，V则是值
    - QKV分别是什么？——数学上是将输入X经过线性变换之后的三种不同状态；三者在数据上都是源于输入特征本身产生的向量QK负责计算注意力权重，V则是X本身的特征
      - 对于Encoder中的MHA，原始qkv是Encoder的Token输入经过Embedding之后的结果，经过线性变换得到输出结果Memory
      - Decoder中的MMHA，原始qkv是Decoder的Token输入经过Embedding之后的结果
      - [深度学习中的注意力模型（2017版） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/37601161)
    - softmax根号d_k的作用：防止softmax值过大，导致导数趋近于0；让注意力分布更加均匀，使得qk更加符合N(0,1)分布，类似归一化，确保稳定性

  - 如何理解多头注意力机制：所谓的多头注意力就是将一个高维度的单头拆分为h个多头，单个头注意力的d_k*h=d_model![img](https://pic2.zhimg.com/80/v2-36ae9333ba8227d1f70bbbc907a97057_720w.webp)
    - 为什么使用多头注意力？如果h越大QKV就被切分的越小，防止注意力权重过度集中在自身位置

  - 如何理解Embedding机制？
    - 仅仅进行Token Embedding在输入语句顺序发生改变的情况下是不会有区别的，因此引入PE，通过引入非常数PE信息之后交换序列位置会有变化
  - Transformer中的EN-DEcode如何理解：
    - Encoder=Muti-Head Attn->Add&Norm->Feed Forward（包含残差）
    - Decoder = Masked MHA+Memory（类Encoder结构但是多了和En交互的部分Encoder-Decoder attention）
      - 这里的KV都是编码部分的Memory线性变换之后的结果，Q是解码部分多头注意力机制输出隐含向量经过线性变换的结果
