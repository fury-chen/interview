# NLP面试模拟问答：

## LLM相关基础问题

- 涌现能力是什么？主要有什么原因？

  - 缩放法则结论：模型的性能强烈依赖于模型的规模——参数量、数据集大小和计算量，即模型的能力可以根据这三个变量来估计，涌现能力指的是，当模型规模超过某个邻接阈值的时候，性能会显著提高到随机，无法通过小规模实验结果观察到的相变
  - 涌现能力的原因：
    - 数据量的增加
    - 计算能力提升
    - 模型架构的改进
    - 预训练和微调的方法改进

- 简要概括一下LLM的处理步骤：

  - word->token（分词）：将sentence通过多种分词方式拆解成最小可分word（BPE、WordPiece、……），此时得到的是一组token string包含n个句子和m个word，然后通过for循环一个一个转换为token_id类似与one-hot
  - Embedding：将词嵌入（映射）到数中，通过Embedding矩阵将(n*m)个token转变为(n * m, Embedding_sz)的嵌入矩阵（张量）
  - SA过程：将word经过Embedding表示后的Tensor经过Transformer模型来获取特征，即通过放缩点积的方法来将q映射到KV空间的过程
    - 这个QKV空间（对输入的三种线性变换形式、三种状态）分别对应的是输入的token和输出的token之间的映射关系表征，并且对位置信息敏感，能够反应这样的一个序列会在哪个token上有注意力较大值，找出对应的next_token
    - 如果采用单头注意力机制的话，会导致KV空间会将注意力集中在自己身上，而忽略了其他位置的可能性，因此这也是采用多头注意力的一点
    - 多头注意力就是将一个注意力空间拆解为多个，将输入空间对应的QKV映射到多个子空间中，这样做的好处是能够克服注意力过于集中于自身的问题
  - Embedding机制：为了捕捉 token在不同位置的特征，需要使用正余弦位置编码
    - Llama_embedding_dim:(32000, 4096, padding = 31999)
  - Norm机制（归一化方法总结）：
    - 归一化的好处：稳定后向的梯度，作用大于稳定输入分布
    - Batch_Norm：主要对数据的一定维度在batch数据中进行归一，一般用于图像数据，不太适合序列数据
    - Layer_Norm：针对序列数据提出的归一方法，主要在layer维度进行归一化，对整个序列进行归一化，LN会计算一个layer所有的activation均值和方差，利用均值和方差进行归一化
      - 为什么不适用BN？在训练的时候BN需要保存每个step的统计信息，在测试时，由于decode特性，测试集可能出现比训练集更长的句子，对于后面位置的step是没有训练统计量使用的；另一种说法是不同句子的长度不同，对所有样本的统计均值是没有意义的
      - 与BN的区别：BN在所有维度上统计所有样本均值，LN是在每个样本统计所有维度的值<img src="https://pic2.zhimg.com/80/v2-beff996a551d1cbc78fbd2bf94df7af5_720w.webp" alt="img" style="zoom:67%;" />
    - RMS Norm：为了提升LN的训练速度提出的，RMS也是一种LN，只是归一化方法不同，使用的事均方根进行归一化
    - L1和L2正则化：
      - 相同点：限制模型学习能力，使模型偏好于权值较小的目标函数，防止过拟合
      - 不同点：L1正则化产生更稀疏的权值矩阵（稀疏表示0更多，只有少数特征对模型有贡献），可用于特征选择，L2主要防止模型过拟合

- Llama模型组成：embeded_tokens、layer_num*decoder、Norm

  ```Python
  LlamaForCausalLM(
    (model): LlamaModel(
      (embed_tokens): Embedding(32000, 4096, padding_idx=31999)
      (layers): ModuleList(
        (0-31): 32 x LlamaDecoderLayer(
          (self_attn): LlamaAttention(
            (q_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (k_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (v_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (o_proj): Linear(in_features=4096, out_features=4096, bias=False)
            (rotary_emb): LlamaRotaryEmbedding()
          )
          (mlp): LlamaMLP(
            (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (down_proj): Linear(in_features=11008, out_features=4096, bias=False)
            (up_proj): Linear(in_features=4096, out_features=11008, bias=False)
            (act_fn): SiLUActivation()
          )
          (input_layernorm): LlamaRMSNorm()
          (post_attention_layernorm): LlamaRMSNorm()
        )
      )
      (norm): LlamaRMSNorm()
    )
    (lm_head): Linear(in_features=4096, out_features=32000, bias=False)
  )
  ```

  

## 马尔科夫模型专题

- HMM专题：

  - 参考链接：[从零开始理解——隐马尔可夫模型（HMM） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/712667284)

  - 马尔科夫假设说明：
    - 有限记忆：下一个过程的状态只和当前状态有关
    - 形式化表示：根据贝叶斯推导
  - 马尔科夫链：一种概率模型，用于表述一个序列当中的事件，其中每个事件的发生只依赖于前一个事件的状态，这是一种强调状态之间转移的概率，而不是单个事件本身，这是随机过程的一种，特别是从状态到另一个状态的改变
  - 隐马尔科夫模型是一种拓展，基于MM的统计模型，包含两个结构（隐状态和观测状态）
    - 隐状态：构成马尔科夫链，但不可观测，只能通过观测状态间接观测到
    - 观测状态：每个隐状态会生成一个观测状态，观测序列是基于隐序列生成的，但观测结果并不直接揭示隐状态，有点类似RNN
  - 生成性质：描述了如何从模型参数生成观测数据序列，首先从初始分布生成第一个状态，然后根据转移矩阵和当前 状态生成下一个状态，同时根据发射分布生成每个状态的观测



## NLP模型专题

- 反向传播相关：[从反向传播推导到梯度消失and爆炸的原因及解决方案（从DNN到RNN，内附详细反向传播公式推导） - 知乎 (zhihu.com)](https://zhuanlan.zhihu.com/p/76772734)
  - 反向传播的梯度更新到底如何更新？——根据损失损失函数的偏导数进行更新

- RNN模型：
  - 模型结构：根据时间步使用同一个隐藏状态进行输出yt = o(xt-1)



## GEMM相关问答

- 

## C++八股整理

- 静态成员函数访问非静态数据成员的方法
- GPT回答：面试的时候，面试官想让你讲解一下面向对象的理解，你会如何作答，从哪些方面入手作答？
  - 基本概念：
    - 定义：面向对象是一种程序设计范式，以对象为中心来组织代码
    - 对象：是指类的实力，包含数据和方法
  - 四大特征：
    - 封装：将数据和数据方法封装在一起，使对象的内部状态对外部隐蔽，通过公开接口来访问和修改属性，从而保护对象的完整性
    - 继承：通过继承机制，子类可以继承父类的属性和方法，促进代码重用和逻辑层次结构建立
    - 多态：允许不同类的对象通过相同接口进行交互，具体实现由对象的类型决定。可以通过方法重载和方法重写来实现
    - 抽象：通过抽象类和接口来定义一组标准，明确对象应该有哪些行为而不实现具体细节

- 讲一下封装、继承、多态的意义：
  - 封装：保护和防止代码在无意中破坏类中的成员，不让类以外的程序直接访问或者修改，只能通过提供的公共接口访问
  - 继承：继承的事基类的全体数据和成员函数，具有基类复制而来的数据成员和成员函数（基类私有成员可以被继承，但是无法被访问），提高了程序的复用效率
    - 不能被继承的：构造、析构、友元、静态数据成员、静态成员函数
  - 多态：不同继承的对象对同一消息做出不同的响应，基类指针指向或绑定到派生类的对象，使得基类指针呈现不同表现形式，对已存在的代码具有可替代性，代码具有可扩充性，新增子类不会影响已存在类的各种性质，简化对应用代码的编写和修改

## 基础训练相关：

- warm up一般在什么情况下使用：
  - 优化器是在模型训练过程中用来更新模型参数，最小化或者最大化损失函数，以提升模型效果
  - 优化器主要依赖两个条件：学习率和梯度
  - warm up：优化器本身是根据梯度来调整学习率的，一般刚开始训练学习的时候梯度很大，所以学习率也比较大，但是有些情况下，使用了预训练模型进行下游任务的时候，学习率太大会导致模型不稳定，使得模型发生震荡，因此需要让刚开始训练的时候有一个较小的学习率，确保模型的良好收敛性。warm up就是通过学习率从零开始增加，增加到warm up的设定值在逐渐开始减小

- 梯度爆炸要怎么解决：
  - pretraining + finetuning：寻找局部最优，然后整合到全局最优
  - 梯度裁剪
  - 权重正则化
  - 选择relu等梯度落在常数上的激活函数
  - 残差
  - LSTM
- 
